{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q82ae19kDpbW"
      },
      "source": [
        "## Import Packages\n",
        "\n",
        "First, let's import the packages we will need for this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VVS98B1VKzBj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import glob\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLWPNzNjLc4p",
        "outputId": "74249033-ee51-4fd1-dba8-3f6d4ae32419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install roboflow supervision -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BPT7w3mEKrf",
        "outputId": "a7d7d475-f443-422a-c724-f5c347b36fab"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\rvisit https://app.roboflow.com/auth-cli to get your authentication token.\n"
          ]
        }
      ],
      "source": [
        "import roboflow\n",
        "import supervision as sv\n",
        "\n",
        "roboflow.login()\n",
        "\n",
        "rf = roboflow.Roboflow()\n",
        "\n",
        "project = rf.workspace(\"popular-benchmarks\").project(\"mit-indoor-scene-recognition\")\n",
        "dataset = project.version(5).download(\"folder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "064ynDDhCU2i"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "\n",
        "ROOT_DIR = os.path.join(cwd, \"MIT-Indoor-Scene-Recognition-5/train\")\n",
        "\n",
        "labels = {}\n",
        "\n",
        "for folder in os.listdir(ROOT_DIR):\n",
        "    for file in os.listdir(os.path.join(ROOT_DIR, folder)):\n",
        "        if file.endswith(\".jpg\"):\n",
        "            full_name = os.path.join(ROOT_DIR, folder, file)\n",
        "            labels[full_name] = folder\n",
        "\n",
        "files = labels.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF7rqN44CVLV"
      },
      "outputs": [],
      "source": [
        "dinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dinov2_vits14.to(device)\n",
        "\n",
        "#transform_image = T.Compose([T.ToTensor(), T.Resize(244), T.CenterCrop(224), T.Normalize([0.5], [0.5])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0woC0J7CX-z"
      },
      "outputs": [],
      "source": [
        "def load_image(img: str) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Load an image and return a tensor that can be used as an input to DINOv2.\n",
        "    \"\"\"\n",
        "    img = Image.open(img)\n",
        "\n",
        "    # **Use bicubic interpolation for smoother transitions**\n",
        "    transform_image = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Resize(244, interpolation=T.InterpolationMode.BICUBIC),  # **Change here**\n",
        "        T.CenterCrop(224),\n",
        "        T.Normalize([0.5], [0.5])\n",
        "    ])\n",
        "\n",
        "    transformed_img = transform_image(img)[:3].unsqueeze(0)\n",
        "\n",
        "    return transformed_img\n",
        "\n",
        "def compute_embeddings(files: list) -> dict:\n",
        "    \"\"\"\n",
        "    Create an index that contains all of the images in the specified list of files.\n",
        "    \"\"\"\n",
        "    all_embeddings = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i, file in enumerate(tqdm(files)):\n",
        "        embeddings = dinov2_vits14(load_image(file).to(device))\n",
        "\n",
        "        all_embeddings[file] = np.array(embeddings[0].cpu().numpy()).reshape(1, -1).tolist()\n",
        "\n",
        "    with open(\"all_embeddings.json\", \"w\") as f:\n",
        "        f.write(json.dumps(all_embeddings))\n",
        "\n",
        "    return all_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDMx6fBVCqib"
      },
      "outputs": [],
      "source": [
        "embeddings = compute_embeddings(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHrpE_-pCu9z"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "\n",
        "clf = svm.SVC(gamma='scale')\n",
        "\n",
        "y = [labels[file] for file in files]\n",
        "\n",
        "embedding_list = list(embeddings.values())\n",
        "\n",
        "clf.fit(np.array(embedding_list).reshape(-1, 384), y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peRLdL1DC5lN"
      },
      "outputs": [],
      "source": [
        "input_file = \"MIT-Indoor-Scene-Recognition-5/test/elevator/elevator_google_0053_jpg.rf.41487c3b9c1690a5de26ee0218452627.jpg\"\n",
        "\n",
        "new_image = load_image(input_file)\n",
        "\n",
        "%matplotlib inline\n",
        "#sv.plot_image(image=new_image, size=(16, 16))\n",
        "\n",
        "# Convert the PyTorch tensor to a NumPy array and then to the correct data type\n",
        "#new_image_np = new_image.cpu().numpy().squeeze()\n",
        "#new_image_np = new_image_np.transpose((1, 2, 0))\n",
        "#new_image_np = (new_image_np * 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "new_image_np = new_image.cpu().numpy().squeeze()\n",
        "new_image_np = new_image_np.transpose((1, 2, 0))\n",
        "\n",
        "\n",
        "new_image_np = (new_image_np * 0.5 + 0.5) * 255  # Reverse normalization\n",
        "new_image_np = new_image_np.astype(np.uint8)\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "# Use the converted NumPy array for plotting\n",
        "sv.plot_image(image=new_image_np, size=(16, 16))\n",
        "\n",
        "with torch.no_grad():\n",
        "    embedding = dinov2_vits14(new_image.to(device))\n",
        "\n",
        "    prediction = clf.predict(np.array(embedding[0].cpu()).reshape(1, -1))\n",
        "\n",
        "    print()\n",
        "    print(\"Predicted class: \" + prediction[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}